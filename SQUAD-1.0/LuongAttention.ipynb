{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models.Attention import LuongAttention\n",
    "from utils.data_reader import load_and_preprocess_data, load_word_embeddings\n",
    "from utils.result_saver import ResultSaver\n",
    "from os.path import join as pjoin\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.eval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.001, \"Learning rate\")\n",
    "tf.app.flags.DEFINE_float(\"keep_prob\", 0.8, \"The probably that a node is kept after the affine transform\")\n",
    "tf.app.flags.DEFINE_float(\"max_grad_norm\", 5.,\n",
    "                          \"The maximum grad norm during backpropagation, anything greater than max_grad_norm is truncated to be max_grad_norm\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 24, \"Number of batches to be used per training batch\")\n",
    "tf.app.flags.DEFINE_integer(\"eval_num\", 250, \"Evaluate on validation set for every eval_num batches trained\")\n",
    "tf.app.flags.DEFINE_integer(\"embedding_size\", 100, \"Word embedding size\")\n",
    "tf.app.flags.DEFINE_integer(\"window_size\", 3, \"Window size for sampling during training\")\n",
    "tf.app.flags.DEFINE_integer(\"hidden_size\", 100, \"Hidden size of the RNNs\")\n",
    "tf.app.flags.DEFINE_integer(\"samples_used_for_evaluation\", 500,\n",
    "                            \"Samples to be used at evaluation for every eval_num batches trained\")\n",
    "tf.app.flags.DEFINE_integer(\"num_epochs\", 10, \"Number of Epochs\")\n",
    "tf.app.flags.DEFINE_integer(\"max_context_length\", None, \"Maximum length for the context\")\n",
    "tf.app.flags.DEFINE_integer(\"max_question_length\", None, \"Maximum length for the question\")\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", \"data/squad\", \"Data directory\")\n",
    "tf.app.flags.DEFINE_string(\"train_dir\", \"\", \"Saved training parameters directory\")\n",
    "tf.app.flags.DEFINE_string(\"retrain_embeddings\", False, \"Whether or not to retrain the embeddings\")\n",
    "tf.app.flags.DEFINE_string(\"share_encoder_weights\", False, \"Whether or not to share the encoder weights\")\n",
    "tf.app.flags.DEFINE_string(\"learning_rate_annealing\", False, \"Whether or not to anneal the learning rate\")\n",
    "tf.app.flags.DEFINE_string(\"ema_for_weights\", False, \"Whether or not to use EMA for weights\")\n",
    "tf.app.flags.DEFINE_string(\"log\", True, \"Whether or not to log the metrics during training\")\n",
    "tf.app.flags.DEFINE_string(\"optimizer\", \"adam\", \"The optimizer to be used \")\n",
    "tf.app.flags.DEFINE_string(\"model\", \"BiDAF\", \"Model type\")\n",
    "tf.app.flags.DEFINE_string(\"find_best_span\", True, \"Whether find the span with the highest probability\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "train, val = load_and_preprocess_data(FLAGS.data_dir)\n",
    "\n",
    "# load the word matrix\n",
    "embeddings = load_word_embeddings(FLAGS.data_dir)\n",
    "\n",
    "# vocab map file\n",
    "vocabs = []\n",
    "with open(pjoin(FLAGS.data_dir, \"vocab.dat\")) as f:\n",
    "    for line in f:\n",
    "        vocabs.append(line.strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the result save (isn't used but is needed to initialize the model should be refactored out to be a \n",
    "# Singleton class)\n",
    "result_saver = ResultSaver(FLAGS.train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:('----------', 'ENCODING ', '----------')\n",
      "INFO:root:('----------', ' DECODING ', '----------')\n",
      "INFO:root:answer_span_start_one_hot.get_shape() <unknown>\n",
      "INFO:root:answer_span_end_one_hot.get_shape() <unknown>\n"
     ]
    }
   ],
   "source": [
    "# model = LuongAttention(result_saver, embeddings, FLAGS)\n",
    "model = LuongAttention(result_saver, embeddings, FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a random sample from the validation set\n",
    "n_val_samples = len(val[\"context\"])\n",
    "\n",
    "index = np.random.choice(np.arange(n_val_samples))\n",
    "\n",
    "sample_data = {}\n",
    "for k, v in val.items():\n",
    "    sample_data[k] = v[[index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The context paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Srijana Contemporary Art Gallery , located inside the Bhrikutimandap Exhibition grounds , hosts the work of contemporary painters and sculptors , and regularly organizes exhibitions . It also runs morning and evening classes in the schools of art . Also of note is the Moti Azima Gallery , located in a three storied building in Bhimsenthan which contains an impressive collection of traditional utensils and handmade dolls and items typical of a medieval Newar house , giving an important insight into Nepali history . The J Art Gallery is also located in Kathmandu , near the Royal Palace in Durbarmarg , Kathmandu and displays the artwork of eminent , established Nepali painters . The Nepal Art Council Gallery , located in the Babar Mahal , on the way to Tribhuvan International Airport contains artwork of both national and international artists and extensive halls regularly used for art exhibitions .\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \" \".join([word for word in sample_data[\"word_context\"]])\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The question to be answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What art gallery is located close to the Durbarmarg Royal Palace ?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what the question is\n",
    "question = \" \".join([vocabs[word] for word in sample_data[\"question\"][0]])\n",
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'J\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = \" \".join([word for word in sample_data[\"word_answer\"]])\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the answer with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from README-files/Attention-model/BATCH-6039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from README-files/Attention-model/BATCH-6039\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, pjoin(\"README-files\", \"Attention-model\", \"BATCH-6039\"))\n",
    "    start_index, end_index = model.answer(sess, sample_data, FLAGS.find_best_span)\n",
    "    pred, truth = model.get_sentences_from_indices(sample_data, start_index, end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from model is: J Art Gallery\n",
      "Ground truth is: J\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction from model is: {}\".format(\" \".join(pred)))\n",
    "print(\"Ground truth is: {}\".format(\" \".join(truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "squad",
   "language": "python",
   "name": "squad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
